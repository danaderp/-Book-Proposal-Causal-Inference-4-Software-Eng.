\chapter{Introduction} % The asterisk * leaves out this chapter from the table of contents
\label{ch:intro}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Paragraph1: Motivation
\newthought{This dissertation} explores the usage of a mathematical structure that assess the causal effect of automation process in the context of Software Engineering. Such mathematical structure embodies the causal calculus to perform estimations of software variables affecting other variables. However, the software variables under analysis are not coming from the classical perspective of software engineering but from the field where Software Engineering is generated by Artificial Intelligence mechanism. This field is introduced as \textit{Artificial Software Engineering} (\asofte). In order to \textit{Artificial Software Engineering} achieve understandability (or interpretability in a Machine Learning context). It is required to adapt, formalize, and evaluate Causal Inference concepts or causal mathematical structures that help aid to uncover causal effects. Therefore, we need a causal artificial software structure at the interface of causality and artificial software engineering.  

% Paragraph2: What is the specific problem?
Although Causal Calculus has been introduced since the 80s, there no exist a formalization of a causal artificial software structure...  

% Paragraph3: What is the main contribution of the dissertation?
This dissertation poses a causal structure for the problem of deep code retrieval and deep code interpretability...

% Paragraph4: Differences of what I am doing and others have done

% Paragraph5: The structure of the dissertation.





%------------------------------------------------
\section{A Motivating Example}

%------------------------------------------------
\section{Terminology}

\subsection{Interpretability}
Neural Language Models (NLM) are increasingly being used in Software Engineering as \textit{Code Generators} showing promising results in generating correct and realistic code. Intepretability represents one of the major challenge limiting the deployment and usage of these models in practice, since the causal relationship between the input and output is often unclear and no cues are provided informing what influenced the generation of a specific snippet of code. In this patent we propose \codeSeqRational, a framework that allows to extract practical interpretability insights for NLM-based systems for code-related tasks. Our approach is based on a greedy algorithm which extracts the smallest subset of tokens (rationales) from the input sufficient to predict each token in the output. Next, these rationales are mapped into human-interpretable concepts by a set of mapping functions, which assign tokens to a set of categories. These include code-specific categories (\ie structural and identifiers extracted with a code parser), as well as natural language categories (\eg verbs and nouns extracted with a NL context-free grammar parser). Finally, tokens are grouped into location-aware scopes. This infrastructure allows researchers and practitioners to debug NLM outputs as well as further optimize the input to these models evaluating the importance of each token, category, or scope.