\section{Deep Code Retrieval Problem}
\label{sec:deep-retrieval-problem}

\david{introduce what software retrieval means. Rewrite the related work to give a proper introduction of the concept}

We focus our discussion of related work on prior techniques that have, in limited contexts, (i) considered novel or hybrid textual similarity measures, (ii) modeled the effects of multiple types of artifacts, or (iii) incorporated developer expertise. We then conclude with a statement distilling \Comets novelty.

\noindent{\textbf{Novel/Hybird Textual Similarity Measures:}} Guo \etal~\cite{Guo:ICSE'17} proposed an approach for candidate trace link prediction that uses a semantically enhanced similarity measure based on Deep Learning (DL) techniques. However, unlike \Comet, this technique requires pre-existing trace links in order to train the DL classifier.  In contrast, \Comet does not require known links for the projects it is applied to, but rather requires a project to serve as a tuning set. We show that \Comet performs well when tuned and tested on different datasets, outperforming Guo \etals DL-based approach when it is trained in a similar manner. Gethers \etal~\citep{Gethers:ICSM'11}, implemented an approach that is capable of combining information from canonical IR techniques (\ie VSM, Jensen-Shannon) with Topic Modeling techniques. However, their approach can only combine two IR/ML techniques, whereas \Comet can combine and leverage the observations from several IR/ML techniques, and combine this with other information such as expert feedback and transitive links. 

\noindent{\textbf{Modeling of Multiple Artifacts:}} Rath \etal~\citep{Rath:ICSE'18} recently explored linking nontraditional information including issues and commits, and Cleland-Huang \etal~\citep{Cleland-Huang:ICSE'10} have investigated linking regulatory codes to product level requirements.  \Comets model has the potential to improve trace link recovery in these scenarios both through its more robust modeling of textual similarity, and through incorporation of transitive link information. Furtado \etal~\cite{Furtado:RE'16}, explored traceability in the context of agile development, and Nishikawa \etal~\citep{Nishikawa:ICSME'15} first explored the use of transitive links in a deterministic traceability model. Additionally, Kuang \etal used the closeness of code dependencies, to help improve IR-based traceability recovery~\citep{Kuang:SANER'17}. However, none of these approaches is capable of incorporating transitive links while also considering combined textual similarity metrics and developer feedback.

\noindent{\textbf{Incorporation of Developer Expertise:}} De Lucia \etal \citep{DeLucia:ICSM'06} and Hayes \etal~\citep{Hayes:TSE'06} analyzed approaches that use relevance feedback to improve trace link recovery. However, these approaches are either tied to a particular type of model (such as TF-IDF~\citep{DeLucia:ICSM'06}), or require knowledge of the underlying model to function optimally. In contrast, \Comet implements a lightweight, likert-based feedback collection mechanism that we illustrate can improve link accuracy even when only a small amount of feedback is collected.
 
\noindent{\textbf{Summary of Advancement over Prior Work:}} \Comets features facilitate its application to projects without any pre-existing trace links, and as our evaluation illustrates, allow it to perform consistently well across datasets. \Comet is able to combine information from transitive links with both robust textual similarity measures and lightweight developer feedback for improved accuracy. While some aspects of \Comets approach have been considered in limited contexts in prior work -- such as developer feedback~\citep{DeLucia:ICSM'06,Hayes:TSE'06} and restricted combinations of IR/ML techniques~\citep{Gethers:ICSM'11} -- there has never been a framework capable of combining all these aspects in a holistic approach. Our evaluation illustrates that \Comets holistic HBN is able to outperform baseline techniques on average.

\subsection{Formalization}

Our goal is to design a model that captures meaningful information regarding logical relationships between software artifacts, and then use this model to infer a set of candidate trace links. More specifically, given a set of source artifacts $S$ (\eg requirements, use cases) such that $S = \{S_{1},S_{2},\ldots S_{n}\}$ and a set of target artifacts $T$ (\eg source code files, test cases) such that $T = \{T_{1},T_{2},\ldots T_{n}\}$, we aim to infer whether a trace link $L$ exists between all possible pairs of artifacts in $S$ and $T$ such that $L = \{(s,t) | s\in S, t\in T, s\leftrightarrow t\}$ where each pair of artifacts $s$ and $t$ are said to be logical trace links.


\subsection{The Probabilistic Nature of Software Traceability}
\david{The probabilistic nature of software retrieval!}
The process of building software is not inherently deterministic, and is instead the result of decisions made by engineers over prolonged periods of time that may be hard to predict. Developer decisions related to nearly every observable phenomenon in modern software development are influenced by a combination of multiple factors. For instance, the presence of a functional bug may be influenced by the quality of related requirements, implementation constraints imposed by a given programming language~\citep{Ray:CACM'17}, or the change-proneness of underlying APIs~\citep{Linares-Vasquez:FSE'13}. Given that such factors are often hard to predict, there is a clear sense of randomness inherent to the software development process. Similarly, the existence of trace links among software artifacts is also likely to be influenced by several different effectively \textit{random} factors.  

These factors could include textual similarities between artifacts, programmatic associations between pieces of code, or even abstract notions of similarity held by expert developers.  For example, the textual quality of requirements or identifiers in code are typically a function of several factors such as the fluency and writing style of the author and the familiarity of key phrases chosen for identifiers \citep{Dasgupta:ICSME'13}. This may lead to variable names that may be perfectly clear to one engineer being indecipherable to another.  \textit{From this view point, the existence of trace links between software artifacts can be thought of as an inherently a probabilistic phenomenon}.

\subsection{Traceability as a Bayesian Inference Problem}

Hence, in order to effectively model trace links among software artifacts, it is necessary to model a collection of random factors that influence \textit{the probability that a trace link exists}. Thus, the process of deriving trace links can be modeled as a \textit{Bayesian inference} problem, wherein a probability distribution representing the existence of a trace link between two artifacts can be inferred. As we illustrate, by modeling the trace link recovery problem in a probabilistic manner, we are able to construct an an automated approach that largely overcomes the typical drawbacks discussed in \chapref{ch:hbn}. To understand this context, let us consider the general definition of Bayes' Theorem:

%\marginnote{
\begin{equation}
P(H|O) = \frac{P(O|H)\cdot P(H)}{P(O)}
\end{equation}
%}

\noindent where $H$ is a hypothesis regarding some phenomenon, $O$ is a set of observations that provide some information about the hypothesis, and where our goal is to infer or estimate the probability that our hypothesis is true $P(H|O)$, which is called the \textit{posterior probability distribution}, or more simply the \textit{posterior}. However, a given hypothesis is rarely made in a vacuum, and one typically holds some \textit{prior belief} as to the probability that is being inferred.  This prior belief is modeled as a probability distribution $P(H)$, which we will simply refer to as the \textit{prior}, and can be influenced by a number of factors. In order for the posterior to be inferred from a set of observations, these must be modeled in a probabilistic manner. This is the purpose of the \textit{likelihood} $P(O|H)$, which is a probability distribution that is derived purely from observed data. Thus, in Bayesian inference initial beliefs are represented as the prior, observations are modeled as the likelihood and the final beliefs are represented by the posterior. This posterior probability distribution can be \textit{inferred} via one of several existing statistical inference techniques. In framing the problem of inferring trace links as a Bayesian problem, we consider our hypothesis to be whether a given trace link exists between a single source artifact $S_x$ and a single target artifact $T_y$. Given the nature of trace links (\eg a link either does or does not exist) we can model our prior as a distribution on the interval $[0,1]$, where 1 indicates the presence of a link and 0 indicates an absence. 

\subsection{A Hierarchical Bayesian Network for Traceability}

In the context of this paper, we will consider our \textit{likelihood} (observations) to be the binary indication that a link exists according to a set of textual similarity measures and an empirically derived threshold value. However, given that we aim to model multiple factors that might influence traceability, our model employs multiple \textit{priors}, called \textit{hyperpriors}, forming a Hierarchical Bayesian Network (HBN). In this work, we consider three priors corresponding to the three factors we wish to model: (i) a normalized set of diverse textual similarity measures, (ii) developer expertise, and (iii) transitive trace links. We assign each of these priors an initial probability distribution, which is then influenced and estimated based upon observable data (e.g. a developer confirming or denying a trace link). Once this network is established, the \textit{posterior} can be computed via one of several estimation techniques. By modeling these three information sources, our technique is able to largely overcome the limitations enumerated in \chapref{ch:hbn}. HBNs are also highly extensible via adjustments to the prior(s). Thus our defined model be capable of adapting to advancements in textual similarity measures or considering new development artifacts from future development workflows.
