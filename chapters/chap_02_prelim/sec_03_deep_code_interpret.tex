\section{Deep Code Interpretability Problem}
\label{sec:deep-interpret-problem}

Accuracy is neither a sufficient nor complete metric to assess deep learning (DL) algorithms. We cannot rely purely on loss values either. The usage of Deep Learning Models and, specifically, Neural Language Models (NLM) in Software Engineering has seen striking advances in code generation and downstream SE tasks \citep{Chen2021EvaluatingCode,Watson2020}. Nonetheless, researchers are unable to establish if such models capture meaning from source code.  \textit{How do we determine if, for instance, a given model is using the inner structures of code to generate code?} What if the outcome of a NLM contains information that was not included in the training set? How do we quantify such information? Can we determine if our deep code models are able to understand causal relationships in the data?  It is required to create additional mathematical and interpretable tools to verify the extent deep neural nets are able to capture meaning. 

Neural Language Models (NLM) have been employed in Software Engineering as \textit{Code Generators}. \textit{Code generation} is a relevant problem within the computer science area as noted by Gulwani et al. \citep{Gulwani2017} and Ouni et al. \citep{Ouni2017}, specially for areas such as software engineering and  machine learning for the automation of tasks such as code completion \citep{Nguyen2013}, program synthesis \citep{Gulwani2017}, program repair \cite{Chen2019} and program translation \citep{Aggarwal2015}. Watson et al. \citep{Watson2020}  highlighted the lack of rigorous statistical analysis when reporting DL models. Such rigor can be achieved by making language models interpretable and explainable. Molnar \citep{Molnar2019} shows that despite the great results achieved by \textit{ML} models, these models usually do not provide an \textit{explanation for their output}, which is a barrier to their adoption.

Doshi-Velez and Kim \citep{Doshi-Velez} define interpretability in the context of \textit{ML} systems as the ``ability to explain or to present in understandable terms to a human". \textit{Interpretable Machine Learning} refers to methods and models that make the behavior and predictions of machine learning systems understandable to humans. Moreover, \textit{Interpretability is about the extent to which a cause and effect can be observed within a system}. That is, the extent to which a stakeholder is able to predict the model outcome, given a change in an input or algorithmic parameters \citep{Molnar2019}.