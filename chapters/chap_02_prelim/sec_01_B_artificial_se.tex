\section{Neural Code Generators}
\label{sec:ncg}

\textbf{Code Representation.} Assuming a train corpus of code data (\eg code snippets) $x \in X$ is represented by a distribution with a form $p_{data}(X)$. A Neural Code Generator (\ncg) is a probability distribution $p_{model}(X)$ \textit{statistically learned} by an autoregressive (\ie Transformer) or recursive (\ie RNN) deep learning model. The question \ncg's attempt to answer is \textit{How can we learn $p_{model}$ similar to $p_{data}$?} Both are joint distributions but $p_{data}$ and $p_{model}$ behave distinctly. On the one hand, we consider $p_{data}$ observational since we are just using samples from code snippets written by humans. On the other hand, $p_{model}$ is a reconstructed probability from generalizing human snippets. 

We use deep generative theory, statistical analysis, information theory, and causal inference to compare machine with human code samples using \textit{interpretability techniques}. The distribution $p_{model}$ can be sampled in two ways: conditioned $p(x|w_{<t},\theta)$ and unconditioned  $p(x|w_0,\theta)$. Note that the unconditioned distribution is approximated by performing semi-supervised learning conditioning on hyperparameters $\theta$ and a special \textit{starting token} $w_0$. If we want to represent a specific deep learning approach generating a sequence of tokens, then it is employed the notation $p_{model}(x|w_0, \theta)$, where the $model$ is any DL architecture. 

Therefore, $p_{data} \approx p_{model}(x|w_0, \theta)$ is an approximation of human generated code. We can condition based on the model parameters $\theta$ and sub-tokens of the code corpora $w$. The learning parameters $\theta$ refers to the variables that affect the generation. Then, the parameters for representing the distribution (i.e. Mixture Models, embeddings, or PCA) are not taken into consideration for the generative process. We obtain a conditional distribution $p(x|w,\theta)$. We say that $p(x|w,\theta)$ is observational because we are estimating $X$ given that we \textbf{observe} variable $w$ takes value $w_{<t}$ and $\Theta$ takes value $\theta$. In this particular case, $w$ is self-contained in the data since $w \subseteq X$ and $\theta$ is assumed to be contain in the train data. In any case, we are passively observing features from a human generated corpora. 

%% Unconditioned and Conditioned Concepts
Furthermore, Neural Code Generators (\ncg) can be split into \textit{\textbf{Unconditioned}} and \textit{\textbf{Conditioned}} models. Conditioned models the generation process receives a sequence as a starting point, thus following a \textit{completion} approach. While unconditioned Language Models (\ulm) follow an open-ended approach: the provided context corresponds to the special \textit{Beginning of Sentence} token.

\textbf{Unconditioned Sampling.} Holtzman et al \citep{Holtzman2019} and Nguyen \citep{Nguyen2021} noted that the \textit{decoding strategy} plays an important role in the generative process of language models (i.e., autoregressive generation). \textit{Decoding strategy} refers to the mechanism used for selecting the output token at each step of the generation process based on the autoregressive models \citep{Holtzman2019}.  At each timestep, the LM produces the probability of each word in the vocabulary being the likely next word to be selected. There are several maximization-based decoding methods such as beam or greedy search, that lead to degeneration of the produced text as noted by Holtzman et al \cite{Holtzman2019}. Holtzman\citep{Holtzman2019} and Nguyen \cite{Nguyen2021} also argued that it is common to leverage \textit{stochastic} decoding methods. These methods aim to introduce a degree of randomness to the generation process, giving the models less chance of repeating themselves. Two popular stochastic decoding methods are \textit{top-k sampling} and \textit{temperature-sampling}.

\textit{Top-k} sampling was originally introduced by Fan et al. \citep{Fan2018}. The authors aimed to train models able to produce coherent and fluent passages of text regarding a topic for the task of \textit{story generation}. The proposed sampling scheme consists of filtering  the $k$ most likely next words and next, redistributing the probability mass among only those $k$ next words. This strategy is sensitive to the choice of parameter $k$.

\textit{Temperature-sampling} refers to a mechanism to shape the distribution resulting from a softmax layer used to pick the next token at each generation step as noted by Ficler and Goldberg \citep{Ficler}. This process aims to increase the likelihood of high probability words and decrease the likelihood of low probability words. Such behavior is attained by modifying the so-called temperature parameter of the softmax function.