\section{Results and Discussion}
\label{sec:results-conditioned}

\subsection{\ref{rq:global} SE Intervention on Global Performance}

\input{tables/tab_2_results_global}

Tab. \ref{tab:results_global} gives an overview of the different associative and interventional effects across our different models and datasets in terms of global performance. Note that the correlation row includes both Pearson and Jensen-Shannon distances. For the \datainterII intervention, the association values in Tab.~\ref{tab:results_global} indicate that Levenshtein ``edit'' distance between Type II clone pairs has a tendency to be positively correlated with cross-entropy values $p(Y_g|T_{dat2})\approx0.60$ for \gru. By contrast, for the Type III clones, no strong correlations were detected between syntactic and global performance differences for any of our \nlms. Nonetheless, we discovered an appreciably strong causal effect between the Levenshtein distance of clone pairs and the difference in cross-entropy with a maximum of $p(Y_g|do(T_{dat2}))=0.87$ for \gru on Type II. This suggests that our models are causally influenced by slight and major changes in syntax of programs such as white space and variable names. This is not a desired effect for code \nlms as developers have their own coding style and conventions, a \nlm should perform similarly independent from the syntactic alterations. 

For \datainterI intervention, it is apparent that both \rnn and \tf exhibit considerably high JS distances. However, after controlling for SE covariates, we found that such correlations were spurious since ATEs are relatively small for the three models. The confounding is explained in Fig.~\ref{fig:covariate}, where the number of subwords per method is affecting the cross entropy directly beyond the \datainterI intervention. We have identified more confounding variables for our proposed interventions such as \textit{the Number of Unique Words}, \textit{MaxNextedBlocks}, and \textit{Cyclomatic Complexity}. However, the variable that most influence prediction performance is the \textit{Number of Subwords} per method. Similar results were found for \datainterIII (TypeIII) interventions after adjustment of covariates. We saw a tendency to null causal effects, meaning, for example, that there is little to no causal influence of removing comments from the code or applying Type III syntactic changes on snippets.

As for model-based interventions, \modelinterI and \modelinterII interventions have a tendency to be negatively correlated to and to causally influence the cross-entropy. For instance, the number of units negatively affect global performance by $p(Y_g|T_{hpy1})\approx-0.084$ for \gru, which is a result align with expected DL experimentation on hyperparameters. Smaller values of cross-entropy are observed once the number of layers and units increases.

\marginnote{
\textit{\ref{rq:global} Global Causal Findings:} 
In contrast to our correlation analysis, after controlling for covariates, we found that \datainterI, \datainterIII, and \datainterII (Type III) had a very small causal effect on cross-entropy across our models. We observed a consistent causal effect on the performance in the presence of syntactic changes (Type II and III) present in our code clone testbeds. Only Transformers had an appreciable correlation and causation between increasing the number of layers and the overall performance in terms of cross-entropy.
}

\subsection{\ref{rq:local} SE Intervention on Local Performance}
\input{tables/tab_3_results_local_jensen}
\input{tables/tab_3_results_local_pearson} 

We measured the robustness of \nlms to syntactic deviations on local performance in Tab.~\ref{tab:resultsLocalJS}. We observed a weak causal effect between clone type variations and Next Token Predictions for all our \nlms across the 10 categories. For instance, on average, semantic Type II changes causally affected the prediction performance of object-oriented tokens $p(Y_{\oop}|do(T_{dat2}))=0.388\pm0.241$ and operators $p(Y_{\operators}|do(T_{dat2}))=0.316\pm0.202$. Semantic type III changes affected blocks of code $p(Y_{\blocks}|do(T_{dat2}))=0.214\pm0.107 $ and declarations $p(Y_{\declarations}|do(T_{dat2}))=0.161\pm0.135$. Importantly, the standard deviation of these categories was quite high across the \nlms suggesting that what the models statistically learned can vary widely. We discovered that \datainterII interventions affected NTP across the categories of our code taxonomy. Specifically, Levenshtein distance between Type II clones had a tendency to be positively correlated with the operators category $p(Y_{\operators}|T_{dat2})\approx0.56$ for \gru. By contrast, for the Type III clones, weak correlations were perceived for Local performance (see Tab.~\ref{tab:resultsLocalJS}). Unfortunately, most of the ATEs cannot be computed for each category because it was not possible to create a linear model to estimate the effects due to the shape of the data (\ie input variables have the same values). As for \datainterI and \datainterIII interventions, $T_{dat0}$ and $T_{dat1}$ treatments had no impact on the prediction of categories in our taxonomy, as their ATEs tend to null causal effects. The maximum causal effect observed for the $do(T_{dat0})$ intervention was $p(Y_{\blocks}|do(T_{dat0}))=-0.000525$ which corresponds with the highest correlation value for blocks category ($p(Y_{\blocks}|T_{dat0})\approx0.6$ for \gru, while the \datainterIII intervention was  $p(Y_{\operators}|do(T_{dat1}))=0.007949$ for \tf. On the other hand, the influence of the number of layers in predicting block's tokens in \gru showed a very low ATE value $p(Y_{\blocks}|do(T_{lyr}))=-0.01$ but the highest correlation $p(p(Y_{\blocks}|T_{lyr})\approx0.7)$ for \tf. The other categories were unable to have their ATE calculated due to limited data. Intervening Transformer layers had only positive correlations across categories. Nonetheless, intervening layers or units in GRUs exhibited mostly negative correlations, with $p(Y_{\exceptions}|T_{unt})=-0.25$ and $p(Y_{\declarations}|T_{lyr})=-0.26$ for \gru.

\marginnote{
\textit{\ref{rq:local} Local Causal Findings:} 
Strong correlation values between \datainterI and \datainterIII are observed for blocks, exceptions, conditionals, and operators for most of the \nlms, in particular, Transformers. As for \modelinterI, we observe strong correlations for block and conditionals. Nonetheless, confounding bias was present in almost all our intervention cases. Possible explanations for this behaviour are found in SE metric covariates. Particularly, the size of the methods influence most of the cross entropy and Next Token Prediction results across the \nlms
}

\subsection{\ref{rq:eval} Stability of Causation}

In addition to the causal effects, we also calculated four different refutation methods where applicable. For a majority of the interventions the causal effects were stable meaning our SCMs for those causal effects were accurate. The only exception to this is for the intervention $T_{dat1}$ (\datainterIII). Specifically, refutations $\mathcal{R}_1$, $\mathcal{R}_2$, and $\mathcal{R}_4$ should have been in the same order of magnitude as ATEs. This could be due to 1) not having enough data samples, 2) incorrect causal diagram (our assumptions of confounders, instrumental vars, and/or effect modifiers are off), and/or 3) our treatment is inadequate. Additionally, for a \datainterII for refutation methods \textit{Placebo} and \textit{Remove Subset} we were unable to compute due to limited data. However, the other two methods, \textit{Random Comm. Cause} and \textit{Unobserved Comm. Cause}, were stable giving us confidence in its ATEs. This stability of our ATEs is especially important for the cases where we found a spurious correlation, \ie $p(Y|T) \ncong p(Y|do(T))$, such as when we found a relatively small effect $p(Y_g|do(T_{lyr}))=-0.01$ for \tf that increases with the number of model layers since it originally had a relatively high correlation $p(Y_g|T_{lyr})\approx-0.4$.

\marginnote{
\textit{\ref{rq:eval} Stability of Causation Findings:} Random common causes and Unobserved Common Causes are the most robust refutation methods of our estimated treatment effects. Placebo effects and Remove Subset refutations methods are difficult to calculate for \datainterII and \modelinterI due to limited data.  
}