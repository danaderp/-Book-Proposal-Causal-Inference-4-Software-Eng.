\section{Case Study Design in Code Generation}\label{sec:design}
%Intro
In order to illustrate the insights that \codegen can enable, we present a case study that serves as a practical application of our interpretability method by analyzing two popular \nlms: RNNs and Transformers. In this section, we detail the methodological steps we took to configure our models and process our datasets. In addition, we designed our methodology for causal understanding following Pearl~\etal's guidelines \citep{Pearl2016Causality,Sharma2021DoWhyAssumptions}. We adopted these guidelines to formulate two research questions: \ref{rq:causal} SE Intervention Effects and \ref{rq:eval} Stability Causation.

\begin{enumerate}[label=\textbf{RQ$_{\arabic*}$:}, ref=\textbf{RQ$_{\arabic*}$}, wide, labelindent=5pt]\setlength{\itemsep}{0.2em}
      \item \label{rq:causal} {\textit{To what extent do SE data and model interventions affect code prediction performance?}} 
       \begin{enumerate}[label=\textbf{RQ$_{1.\arabic*}$:}, ref=\textbf{RQ$_{1.\arabic*}$}, wide, leftmargin=0.5cm]\setlength{\itemsep}{0.2em}
        \item \label{rq:global}{\textit{What is the influence of our interventions on global performance?}}
        \item \label{rq:local}{\textit{What is the influence of our interventions on local performance?}}
        \end{enumerate}
      \item \label{rq:eval} {\textit{How robust are the treatment effects based on SE interventions?}}
\end{enumerate}

\input{tables/tab_1_cases} 

\codegen adapts causal inference theory to aid in providing explanations for both global $Y_g$ (\ref{rq:global}) and local $Y_l$ (\ref{rq:local}) prediction performance of given \nlms. Before \codegen can be performed, a researcher or practitioner making use of our method must select the \nlms and define \textit{hyper-parameter variations} $T_{[hyp]}$ and \textit{data perturbations} $T_{[data]}$ that they would like to examine in Stage 1 ($St_1$). Here, \textit{hyper-parameter variations} are essentially different configurations of a given model according to attributes such as capacity (layers) or the types of layers. Additionally, the researcher must define their code \textit{testbeds}. \codegen offers the possibility of adding additional testbeds with data \textit{perturbations} that represent a set of SE-based interventions or Application Settings. After defining and estimating the causal effect (\ref{rq:causal}) of proposed interventions in Stage 2 ($St_2$), \codegen helps to evaluate the robustness of ATE's results by performing refutation methods proposed in Stage 3 ($St_3$) (\ref{rq:eval}).

\subsection{Context: Data Processing \& Model Training}
To train our \nlms we made use of the Java portion of the commonly used \training Challenge dataset~\citep{husain2019codesearchnet}, which consists of a diverse set of methods (mts) from GitHub \citep{github}. \training is split into a training, validation, and test set. For the testbeds for interpreting the performance of our \nlms using \codegen, we collected four datasets from the \textit{CodeXGLUE} project that contain control and treatment groups \citep{DBLP:journals/corr/abs-2102-04664}. Tab.~\ref{tab:method} shows the generated testbeds containing parallel corpora for understanding the impact of buggy code (\BuggyTB: 64,722 mts), the impact of code documentation (\CommentsTB: 6,664 mts), and syntactic alterations on semantically equivalent snippets based on type II (\BigCloneIITB: 666 mts) and type III (\BigCloneIIITB: 8,097 mts) clones from \BigCloneTB. The only additional filtering we performed on the training, validation, and testbeds was the removal of methods that contain any non-ASCII characters and, for the \CommentsTB, removal of methods that do not have comments.  We derived the uncommented portion of the \CommentsTB by removing any existing comments from the Java test set of the \training dataset.

We performed Byte Pair Encoding (BPE) tokenization \citep{sennrich2015neural} across all testbeds before they were processed by our studied \nlms. BPE has shown to be extremely beneficial for training \nlms on code to overcome the \textit{out-of-vocabulary} problem \citep{Karampatsis2020BigCode}. We trained a BPE tokenizer on 10\% of our training data and used a vocabulary size of 10K. Due to the tokenization process of BPE, some subtokens contained multiple reserved keywords or characters if they appeared frequently together. This was problematic for our interpretability analysis since the function $\phi_{\mathcal{H}}$ relies on the ability to map model predictions to our structural taxonomy. Having certain reserved keywords and tokens merged into BPE subtokens would make it impossible for us to perform this mapping. Therefore, we fixed all reserved keywords and tokens (Fig.~\ref{fig:taxonomy}) to be detected by our trained BPE model.

As for the model training, we used Tensorflow and Pytorch \citep{tensorflow2015-whitepaper, pytorch} (Huggingface's Transformers library \citep{wolf2020transformers}) for creating and training our different models. Our $RNN$ and $TF$ models were trained on \training Java training set. All models reached optimal capacity as they all early terminated to prevent overfitting, with a patience of $5$ epochs without improvement on cross-entropy of at least $1\text{e-2}$. Additionally, we added a \textit{start of sentence} token to the beginning of the input, padded and truncated all inputs to $300$. The training was executed on a 20.04 Ubuntu with an AMD EPYC 7532 32-Core CPU, A100 NVIDA GPU with 40GB VRAM, and 1TB RAM.

\subsection{Case Study Methodology}
We provide an overview of our case study summarized in Tab.~\ref{tab:method}, which follows the \codegen methodology introduced in Sec.~\ref{sec:appII-approach}. This section provides the details regarding how we instantiated \codegen's methodology for our studied models and testbeds. On one hand, we empirically estimated $p(Y|T)$ using two methods: Pearson correlations and Jensen-Shannon Similarities (explained in Def.~\ref{def:js}). On the other hand, we estimated $p(Y|do(T))$ using the \textit{doWhy} library \citep{Sharma2021DoWhyAssumptions} and ATE (explained in Def.~\ref{def:ate}).

\codegen is extensible and researchers can define their own treatments. In our case study, we suggested three \textit{data interventions} $T_{[data]}$ (see row \textit{Data} in Tab.~\ref{tab:method}). In addition, the aim of the syntax analysis was to determine how robust \nlms are in the presence of minor and major syntactic variations of semantically equivalent code snippets. That is, we assessed how semantic preserving changes affect code generation. Since there is no natural split for the different clone types, we were unable to perform a causal analysis on the \BigCloneTB dataset using the typical control and treatment settings. This is because the choice of one clone compared to another is arbitrary when evaluating the set of what \BigCloneTB calls \textit{function$_1$} and \textit{function$_2$} methods. Therefore, instead of our standard covariates, we used the differences between \textit{function$_1$} and \textit{function$_2$} of the different clone types. Specifically, we used the Levenshtein Distance $T_{[dat1]}$, a measure of the difference between two sequences in terms of the necessary edit operations (insert, modify, and remove) needed to convert one sequence into the other, to approximate a treatment where one method was \textit{refactored} into another method by a specific number of edit operations. This formulation allowed us to avoid needing a natural split between \textit{function$_1$} and \textit{function$_2$} and focused on how syntactic differences of methods that perform the same function, \ie semantically similar, affects our models. 

As for the \textit{model interventions} $T_{[hyp]}$ (see row \textit{Model} in Tab.~\ref{tab:method}), our case study consists of an investigation of two different \nlm architectures. We also investigated how layer and unit variations in each architecture affect their performance. We made use of two different types of RNN architectures, a ``base'' model and a model that includes Gated Recurrent Units (GRU). Additionally, we studied a GPT-based Transformer model~\citep{Cho2014GRU, Radford2018ImprovingLU} for our autoregressive model. We chose these two models because RNNs have been extremely popular in SE \citep{watson2020dl4se} and Transformers have recently gained popularity due to the high performance they have achieved in the NLP domain~\citep{Mastropaolo2021StudyingTasks}.