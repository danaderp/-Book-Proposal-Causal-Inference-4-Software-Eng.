\chapter{Understanding Conditioned \hfill \break Neural Code Models}
\label{ch6:conditioned}

%1. Establishing the importance of the field
The combination of large amounts of freely available code-related data, which can be mined from open source repositories, and ever-more sophisticated Neural Code Model (\nlm) architectures have fueled the development of Software Engineering (SE) tools with increasing effectiveness. \nlms have (seemingly) illustrated promising performance across a range of different SE tasks~\citep{Watson:ICSE20,White:MSR15,ciniselli2021empirical,Mastropaolo2021StudyingTasks}. In particular, \textit{code generation} has been an important area of SE research for decades, enabling tools for downstream tasks such as code completion~\citep{MSR-Completion}, program repair~\citep{Chen2019sequencer}, and test case generation~\citep{Watson:ICSE20}. In addition, industry interest in leveraging \nlms for code generation has also grown as evidenced by tools such as Microsoft's IntelliCode \citep{intellicode}, Tabnine \citep{tabnine}, OpenAI's Codex \citep{openai_codex}, and GitHub's Copilot \citep{github_copilot}. Given the prior popularity of code completion engines within IDEs~\citep{murphy2006ide}, and the pending introduction of, and investment in commercial tools, \nlms for code generation will almost certainly be used to help build production software systems in the near future, if they are not being used already.

%2. Presenting the general problem
However, it is generally accepted that \textit{Neural Language Models} operate in a black-box fashion. That is, we are uncertain how these models \textit{arrive at decisions}, which is why \nlms suffer from \textit{incompleteness} in problem formalization \citep{Doshi-Velez2017TowardsLearning}. As such, much of the work on \nlms has primarily relied upon automated metrics (\eg Accuracy, BLEU, METEOR, ROUGE) as an evaluation standard. Skepticism within the natural language processing (NLP) research community is growing regarding the efficacy of current automated metrics ~\citep{ribeiro2020checklist, rei2020comet, kocmi2021ship}, as they tend to overestimate model performance. Even benchmarks that span multiple tasks and metrics have been shown to lack robustness, leading to incorrect assumptions on model comparisons \citep{dehghani2021benchmark}. 

%3. Previous and/or Current Research
Despite the increasing popularity and apparent effectiveness of neural code generation tools, there is still much that is unknown regarding the practical performance of these models, their ability to learn and predict different code-related concepts, and their current limitations. Some of the most popular models for code generation have been adapted from the field of NLP, and thus may inherit the various limitations often associated with such models --- including biases, memorization, and issues with data inefficiency, to name a few~\citep{bender2021parrots}. In fact, recent work from Chen \etal \citep{chen2021evaluating} illustrates that certain issues, such as alignment failures and biases, do exist for large-scale \nlms. Most of the conclusions from Chen~\etal's study were uncovered through manual analysis, \eg through sourcing counterexamples, making it difficult to rigorously quantify or to systematically apply such an analysis to research prototypes~\citep{wu2019errudite}. Given the rising profile and role that \nlms for code generation play in SE, and the current limitations of adopted evaluation techniques, it is clear that new methods are needed that provide deeper insight into \nlms' performance. Notable work has called for a more systematic approach \citep{ribeiro2020checklist} that aims to understand a given model's behavior according to its linguistic capabilities and tests customized for the given task for which a model is applied.

%4. The GaP (or what is missing). Describe the specific problem. Present a prediction to be tested. 
As the discussion above suggests, while it may appear that \nlms have begun to achieve promising performance, it is clearly insufficient to examine \underline{only} prediction values (\ie the \textit{\textbf{what}} of \nlms' decision). This current status quo, at best, provides an incomplete picture of the limitations and caveats of \nlms for code. %As scientists, we allow theories to be falsifiable by means of empirical observations and proper model explanations to avoid pseudo-scientific claims. 
Given the potential impact and consequence of these models and their resulting applications, there is a clear need to strive for a more complete understanding of how they function in practice. As such, we must push to understand how \nlms arrive at their predictions (\ie the \textit{\textbf{why}} of \nlms' decision). In this paper, we cast this problem of achieving a more complete understanding of \nlms as an \textit{Interpretable Machine Learning} task and posit that we can leverage the theory of \textit{causation} as a mechanism to explain \nlms prediction performance. We hypothesize that this mechanism can serve as a useful debugging tool for detecting biases, understanding limitations, and eventually, increasing the reliability and robustness of \nlms employed for the task of code generation \citep{molnar2019interpret,Doshi-Velez2017TowardsLearning}.

%5.Describing the paper itself
This paper introduces \codegen, a novel post-hoc interpretability method specifically designed for understanding the effectiveness of \nlms. The intention of \codegen is to establish a robust and adaptable methodology for \textit{interpreting} predictions of \nlms trained on code in contrast to simply \textit{measuring} the accuracy of these same \nlms. More specifically, \codegen consists of two major conceptual components, (i) a \textit{structural causal graph}, and (ii) a \textit{causal inference mechanism}. \codegen's \textit{structural causal graph} maps model predictions to programming language (PL) concepts and variations in test datasets at different levels of granularity, thus enabling statistical analyses of model predictions rooted in understandable concepts. While our methodology allows for extensiblility in defining relevant PL concepts, we offer an initial \textit{code taxonomy} as a generalizable example. However, examining statistical properties of model predictions in isolation does not provide \textit{explanations} regarding observed performance. As such, the second theoretical component of \codegen adopts a \textit{causal understanding} mechanism that can explain observed trends in model predictions rooted in the aforementioned programming language concepts. This causal inference mechanism allows for the generation of explanations of model performance rooted in \codegen's understandable concepts. Through the introduction of this interpretability framework, we aim to help SE researchers and practitioners by allowing them to understand the potential limitations of a given model, work towards improving models and datasets based on these limitations, and ultimately make more informed decisions about how to build automated developer tools given a more holistic understanding of \textit{\textbf{what}} \nlms are predicting and \textit{\textbf{why}} the predictions are being made.

%5 Announcing Findings
To showcase the types of insights that \codegen can uncover, we perform a case study on different variations of two popular deep learning architectures for the task of code generation, namely RNNs~\citep{RNNs} and Transformers \citep{vaswani2017transformers} trained on the CodeSearchNet dataset~\citep{husain2019codesearchnet}. We instantiated our study using ten structural code categories derived from the Java programming language. This study resulted in several notable findings illustrating the efficacy of our interpretability technique: (i) we find that our studied models learn to predict tokens related to code blocks (\eg brackets, parentheses, semicolons) more effectively than most other code token types (\eg loops, conditionals, datatypes), (ii) we found that our studied models are sensitive to seemingly subtle changes in code syntax, reinforcing previous studies concluding the same~\citep{rabin2021generalizability}, and (iii) our studied models are only marginally impacted by the presence of comments and bugs, which challenges findings from previous work~\citep{Baishakhi2016buggy}.

%------------------------------------------------

\input{chapters/chap_06_interpret-conditioned/sec_1_approach}

%------------------------------------------------

\input{chapters/chap_06_interpret-conditioned/sec_2_design}

%------------------------------------------------

\input{chapters/chap_06_interpret-conditioned/sec_3_results}

%------------------------------------------------

\input{chapters/chap_06_interpret-conditioned/sec_4_discussion}

%------------------------------------------------