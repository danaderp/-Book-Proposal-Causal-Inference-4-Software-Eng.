\section{Adoption \& Challenges of \codegen}
\label{sec:discussion-conditioned}

%Here we show what were the main challenges when performing causal inference and how the community can adopt causal inference for their interpretability methods. 
%For years we have been taught that association does not imply causation, but we will never properly introduced in the statistics of controlling covariates or adjusting confounders. 
This study reconstructs Pearl's theory of Causal Inference and grounds it in interpreting Neural Code Models. Why should we study causation in Deep Learning for Software Engineering? Causation has two main goals in science: discovering causal variables and assessing counterfactual interventions. DL4SE can take advantage from the latter when dealing with uncertainty and confounding bias of \nlms. Estimating counterfactual interventions is a powerful tool to generate explanations of model's performance. Our methodology \codegen can be applied to a wide range of Software Researchers' models for debugging and eliminating confounding bias. However, quantifying the effects requires the causal story underlying data. Randomized controlled experiments were the first option to conduct causality before Pearl's graphical model definitions. Nonetheless, it is not practical to force developers to perform interventions like \datainterII or even train hundreds of \nlms to test treatments. $do-operator$ and causal graphs are useful and better tools to perform causal estimations from observational data. Reconstructing such graphical representation is a challenge since it not only requires formalizing causation in the Software Engineering field (\ie defining potential outcomes, common causes, and treatments) but also tracing and connecting software data to causal models. In addition, formulating interventions is not an easy process. We must hypothesize feasible transformations occurs in code input data to simulate production setting for \nlms. 
%Here are the major threats of our case study: 

%\noindent\textbf{Internal validity:} We controlled for internal validity through defining covariates, computing both correlations and ATE, and computing refutations (some could not be computed since we do not have that much data), but some interventional experiments (\CommentsTB) showed ATE to be unstable. This could be due to 1) not having enough data samples, 2) incorrect causal diagram (our assumptions of confounders, instrumental vars, and/or effect modifiers are off), and/or 3) our treatment is inadequate.

%\noindent\textbf{External validity:} We mitigated external threats by using existing datasets that were well-established, yet our \CommentsTB was artificial and results might not generalize. While we built custom models, the hyperparameter choices were selected from previous studies. We performed various statistical  analyses and reported descriptive statistics for different properties (\ie compatibility intervals).

%\noindent\textbf{Construct validity:}We mitigated these threats by employing interpretability methods to ensure we measure what we intend to measure (long range dependencies for code concepts) and employed grounded functional explanations to evaluate our methodology.
