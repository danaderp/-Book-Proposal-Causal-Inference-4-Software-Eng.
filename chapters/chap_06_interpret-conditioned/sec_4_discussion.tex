\section{Discussion}
\label{sec:discussion-conditioned}

%Here we show what were the main challenges when performing causal inference and how the community can adopt causal inference for their interpretability methods. 
%For years we have been taught that association does not imply causation, but we will never properly introduced in the statistics of controlling covariates or adjusting confounders. 
This study reconstructs Pearl's theory of Causal Inference and grounds it in interpreting Neural Code Models. Why should we study causation in Deep Learning for Software Engineering? Causation has two main goals in science: discovering causal variables and assessing counterfactual interventions. DL4SE can take advantage from the latter when dealing with uncertainty and confounding bias of \nlms. Estimating counterfactual interventions is a powerful tool to generate explanations of model's performance. Our methodology \codegen can be applied to a wide range of Software Researchers' models for debugging and eliminating confounding bias. However, quantifying the effects requires the causal story underlying data. Randomized controlled experiments were the first option to conduct causality before Pearl's graphical model definitions. Nonetheless, it is not practical to force developers to perform interventions like \datainterII or even train hundreds of \nlms to test treatments. $do-operator$ and causal graphs are useful and better tools to perform causal estimations from observational data. Reconstructing such graphical representation is a challenge since it not only requires formalizing causation in the Software Engineering field (\ie defining potential outcomes, common causes, and treatments) but also tracing and connecting software data to causal models. In addition, formulating interventions is not an easy process. We must hypothesize feasible transformations occurs in code input data to simulate production setting for \nlms. 

\subsection{Threats to Validity}
\noindent\textbf{Internal validity:} We controlled for internal validity through defining covariates, computing both correlations and ATE, and computing refutations (some could not be computed since we do not have that much data), but some interventional experiments (\CommentsTB) showed ATE to be unstable. This could be due to 1) not having enough data samples, 2) incorrect causal diagram (our assumptions of confounders, instrumental vars, and/or effect modifiers are off), and/or 3) our treatment is inadequate.

\noindent\textbf{External validity:} We mitigated external threats by using existing datasets that were well-established, yet our \CommentsTB was artificial and results might not generalize. While we built custom models, the hyperparameter choices were selected from previous studies. We performed various statistical  analyses and reported descriptive statistics for different properties (\ie compatibility intervals).

\noindent\textbf{Construct validity:}We mitigated these threats by employing interpretability methods to ensure we measure what we intend to measure (long range dependencies for code concepts) and employed grounded functional explanations to evaluate our methodology.

\subsection{Related Work}
\label{sub:relwork}

\textbf{Applications of LMs in SE:} LMs in SE have a rich history, stemming from the seminal work by Hindle \etal who proposed the concept of the \textit{naturalness} of software using \textit{n}-gram models~\citep{hindle2012natural}. Then, with the rise of Deep Learning, researchers began exploring the possibility of adapting NLMs to code, as initially demonstrated by White \etal~\cite{White:MSR15}. Since then, LMs have been used for a number of SE tasks such as code completion \citep{Nguyen2013ASS, Raychev2014CodeCW, tu2014local, Hellendoorn2017AreCode, Karampatsis2019,  Karampatsis2020Open-VocabularyAbstract, ciniselli2021empirical, chen2021evaluating} and translation \cite{Chen2019sequencer, Roziere2020transcoder, Hu2018comment, Mastropaolo2021StudyingTasks}. 
Researchers have also investigated various representations of LMs for code~\citep{White2016clones} as well as graph-based representations based on ASTs~\cite{allamanis2018learning} and program dependency graphs~\cite{Nguyen:ICSE15}. It has also been shown that using a LM for code representation and then fine-tuning for specific SE tasks can achieve \textit{state-of-the-art} performance across tasks \citep{Hussain2020DeepTL, feng2020codebert, guo2021graphcodebert}. 

\noindent\textbf{Interpretability of LMs:} Many works have investigated evaluating and understanding LMs using various techniques, which can be classified into two basic groups, (i) \textit{explainability techniques} such as probing for specific linguistic characteristics \citep{tenney2019bert} and examining neuron activations~\citep{dai2021knowledge}; and (ii) \textit{interpretability techniques} such as benchmarks \cite{wang2018glue, rajpurkar2018squad, husain2019codesearchnet, lu2021codexglue, chen2021evaluating} and intervention analyses \citep{khandelwal2018sharp, prabhakaran2019perturbation, ribeiro2020checklist, rabin2021generalizability}. Karpathy \etal were among the first to interpret NLMs for code using general Next Token Predictions~\cite{karpathy2015understand} as an interpretability method as well as investigating individual neurons. Our work extends Karpathy \etal's interpretability analysis using causal inference and a more statistically rigorous methodology that grounds its evaluation in SE-specific features and settings.

