\cleardoublepage
\chapter{Abstract} % The asterisk * leaves out this chapter from the table of contents
\label{ch:abstract}

Neural Language Models of Code, or Neural Code Models (NCMs),  are rapidly progressing from research prototypes to commercial developer tools. 
As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of \nlms appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces \codegen, a post-hoc interpretability methodology specific to \nlms that is capable of explaining model predictions. \codegen is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of \codegen are extensible to exploring different model properties, we provide a concrete instantiation that examines \textit{spurious correlations} according to structural properties of programming languages. To demonstrate the practical benefit of \codegen, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and nine \nlms. The results of this case study illustrate that our studied \nlms are sensitive to changes in code syntax and statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of \codegen as a useful model debugging mechanism that may aid in discovering biases and limitations in \nlms.